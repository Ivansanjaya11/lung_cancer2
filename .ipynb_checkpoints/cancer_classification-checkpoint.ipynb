{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d426258-2e34-448b-9eab-5a5c08b3cb24",
   "metadata": {},
   "source": [
    "proposed models: mobilenet, mobilenetv2, mobilenetv3, nasnetmobile, efficientnetv2b0, efficientnetlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb92736-67cf-4b6a-a3b3-126d0d45b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn import metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, NearMiss\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import ResNet50V2, InceptionResNetV2, NASNetMobile, MobileNet\n",
    "from tensorflow.keras.applications import EfficientNetV2B0, EfficientNetV2B1, EfficientNetV2B2, EfficientNetV2B3\n",
    "from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from keras.regularizers import l2\n",
    "import keras_tuner as kt\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "num_classes=4\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "categories = [\"adenocarcinoma\", \"large_cell_carcinoma\", \"normal\", \"squamous_cell_carcinoma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0909c566-1ab5-4b2d-ae3f-c8f594812fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN data\n",
    "\n",
    "directory_train = r\"lung_classification_dataset\\train\"\n",
    "\n",
    "adenocarcinoma_dir = directory_train + \"/\" + categories[0]\n",
    "large_cell_carcinoma_dir = directory_train + \"/\" + categories[1]\n",
    "normal_dir = directory_train + \"/\" + categories[2]\n",
    "squamos_cell_carcinoma_dir = directory_train + \"/\" + categories[3]\n",
    "\n",
    "path_list_train = []\n",
    "category_list_train = []\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(directory_train, category)\n",
    "    folder_image_contents = os.listdir(folder_path)\n",
    "    for image_file in folder_image_contents:\n",
    "        image_file_path = os.path.join(folder_path, image_file)\n",
    "        path_list_train.append(image_file_path)\n",
    "        category_list_train.append(category)\n",
    "\n",
    "path_series_train = pd.Series(path_list_train, name=\"filepath\")\n",
    "category_series_train = pd.Series(category_list_train, name=\"category\")\n",
    "\n",
    "image_paths_train_df = pd.DataFrame(path_series_train).join(category_series_train)\n",
    "\n",
    "#print(image_paths_train_df.head())   \n",
    "\n",
    "# VAL data\n",
    "directory_val = r\"lung_classification_dataset\\valid\"\n",
    "\n",
    "adenocarcinoma_dir = directory_val + \"/\" + categories[0]\n",
    "large_cell_carcinoma_dir = directory_val + \"/\" + categories[1]\n",
    "normal_dir = directory_val + \"/\" + categories[2]\n",
    "squamos_cell_carcinoma_dir = directory_val + \"/\" + categories[3]\n",
    "\n",
    "path_list_val = []\n",
    "category_list_val = []\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(directory_val, category)\n",
    "    folder_image_contents = os.listdir(folder_path)\n",
    "    for image_file in folder_image_contents:\n",
    "        image_file_path = os.path.join(folder_path, image_file)\n",
    "        path_list_val.append(image_file_path)\n",
    "        category_list_val.append(category)\n",
    "\n",
    "path_series_val = pd.Series(path_list_val, name=\"filepath\")\n",
    "category_series_val = pd.Series(category_list_val, name=\"category\")\n",
    "\n",
    "image_paths_val_df = pd.DataFrame(path_series_val).join(category_series_val)\n",
    "\n",
    "#print(image_paths_val_df.head())\n",
    "\n",
    "# TEST data\n",
    "directory_test = r\"lung_classification_dataset\\test\"\n",
    "\n",
    "adenocarcinoma_dir = directory_test + \"/\" + categories[0]\n",
    "large_cell_carcinoma_dir = directory_test + \"/\" + categories[1]\n",
    "normal_dir = directory_test + \"/\" + categories[2]\n",
    "squamos_cell_carcinoma_dir = directory_test + \"/\" + categories[3]\n",
    "\n",
    "path_list_test = []\n",
    "category_list_test = []\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(directory_test, category)\n",
    "    folder_image_contents = os.listdir(folder_path)\n",
    "    for image_file in folder_image_contents:\n",
    "        image_file_path = os.path.join(folder_path, image_file)\n",
    "        path_list_test.append(image_file_path)\n",
    "        category_list_test.append(category)\n",
    "\n",
    "path_series_test = pd.Series(path_list_test, name=\"filepath\")\n",
    "category_series_test = pd.Series(category_list_test, name=\"category\")\n",
    "\n",
    "image_paths_test_df = pd.DataFrame(path_series_test).join(category_series_test)\n",
    "\n",
    "#print(image_paths_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb24c56-1a84-4656-aa0e-b8abaa5b14c6",
   "metadata": {},
   "source": [
    "# Image resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11f0e1d-58a4-4b4d-9348-32022d420471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN data\n",
    "size_data_train = {}\n",
    "is_grayscale = 0 # enter 0 for color, 1 for grayscale\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(directory_train, category)\n",
    "    folder_image_contents = os.listdir(folder_path)\n",
    "    image_sizes_in_one_category = {}\n",
    "    for image_file in folder_image_contents:\n",
    "        image_file_path = os.path.join(folder_path, image_file)\n",
    "        img = cv2.imread(image_file_path)\n",
    "        height, width, channel = img.shape\n",
    "        if(str(height) + \" x \" + str(width) in image_sizes_in_one_category):\n",
    "            image_sizes_in_one_category[str(height) + \" x \" + str(width)] += 1\n",
    "        else:\n",
    "            image_sizes_in_one_category[str(height) + \" x \" + str(width)] = 1\n",
    "    #plt.imshow(cv2.imread(image_file_path, is_grayscale))\n",
    "    #plt.show()\n",
    "    size_data_train[category] = image_sizes_in_one_category\n",
    "size_data_train_df = pd.DataFrame(size_data_train)\n",
    "#print(size_data_train)\n",
    "#print(size_data_train_df.head(10))\n",
    "\n",
    "# VAL data\n",
    "size_data_val = {}\n",
    "is_grayscale = 0 # enter 0 for color, 1 for grayscale\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(directory_val, category)\n",
    "    folder_image_contents = os.listdir(folder_path)\n",
    "    image_sizes_in_one_category = {}\n",
    "    for image_file in folder_image_contents:\n",
    "        image_file_path = os.path.join(folder_path, image_file)\n",
    "        img = cv2.imread(image_file_path)\n",
    "        height, width, channel = img.shape\n",
    "        if(str(height) + \" x \" + str(width) in image_sizes_in_one_category):\n",
    "            image_sizes_in_one_category[str(height) + \" x \" + str(width)] += 1\n",
    "        else:\n",
    "            image_sizes_in_one_category[str(height) + \" x \" + str(width)] = 1\n",
    "    #plt.imshow(cv2.imread(image_file_path, is_grayscale))\n",
    "    #plt.show()\n",
    "    size_data_val[category] = image_sizes_in_one_category\n",
    "size_data_val_df = pd.DataFrame(size_data_val)\n",
    "#print(size_data_val)\n",
    "#print(size_data_val_df.head(10))\n",
    "\n",
    "# TEST data\n",
    "size_data_test = {}\n",
    "is_grayscale = 0 # enter 0 for color, 1 for grayscale\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(directory_test, category)\n",
    "    folder_image_contents = os.listdir(folder_path)\n",
    "    image_sizes_in_one_category = {}\n",
    "    for image_file in folder_image_contents:\n",
    "        image_file_path = os.path.join(folder_path, image_file)\n",
    "        img = cv2.imread(image_file_path)\n",
    "        height, width, channel = img.shape\n",
    "        if(str(height) + \" x \" + str(width) in image_sizes_in_one_category):\n",
    "            image_sizes_in_one_category[str(height) + \" x \" + str(width)] += 1\n",
    "        else:\n",
    "            image_sizes_in_one_category[str(height) + \" x \" + str(width)] = 1\n",
    "    #plt.imshow(cv2.imread(image_file_path, is_grayscale))\n",
    "    #plt.show()\n",
    "    size_data_test[category] = image_sizes_in_one_category\n",
    "size_data_test_df = pd.DataFrame(size_data_test)\n",
    "#print(size_data_test)\n",
    "#print(size_data_test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e7b015-e2b2-4457-8405-9accdbc66004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN data\n",
    "\n",
    "dataset_train = []\n",
    "desired_size = 256\n",
    "for index, row in image_paths_train_df.iterrows():\n",
    "    filepath = row[\"filepath\"]\n",
    "    img = cv2.imread(filepath, is_grayscale)\n",
    "    img = cv2.resize(img, (desired_size, desired_size))\n",
    "    label = row[\"category\"]\n",
    "    dataset_train.append([img, label])\n",
    "    \n",
    "random.shuffle(dataset_train)\n",
    "\n",
    "#print(dataset_train[0][0].shape)\n",
    "#print(dataset_train[0][1])\n",
    "#plt.imshow(dataset_train[0][0])\n",
    "#plt.show()\n",
    "\n",
    "# VAL data\n",
    "\n",
    "dataset_val = []\n",
    "desired_size = 256\n",
    "for index, row in image_paths_val_df.iterrows():\n",
    "    filepath = row[\"filepath\"]\n",
    "    img = cv2.imread(filepath, is_grayscale)\n",
    "    img = cv2.resize(img, (desired_size, desired_size))\n",
    "    label = row[\"category\"]\n",
    "    dataset_val.append([img, label])\n",
    "    \n",
    "random.shuffle(dataset_val)\n",
    "\n",
    "#print(dataset_val[0][0].shape)\n",
    "#print(dataset_val[0][1])\n",
    "#plt.imshow(dataset_val[0][0])\n",
    "#plt.show()\n",
    "\n",
    "# TEST data\n",
    "\n",
    "dataset_test = []\n",
    "desired_size = 256\n",
    "for index, row in image_paths_test_df.iterrows():\n",
    "    filepath = row[\"filepath\"]\n",
    "    img = cv2.imread(filepath, is_grayscale)\n",
    "    img = cv2.resize(img, (desired_size, desired_size))\n",
    "    label = row[\"category\"]\n",
    "    dataset_test.append([img, label])\n",
    "    \n",
    "random.shuffle(dataset_test)\n",
    "\n",
    "#print(dataset_test[0][0].shape)\n",
    "#print(dataset_test[0][1])\n",
    "#plt.imshow(dataset_test[0][0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ae7cf-a57b-4faa-8a46-ba54075168f2",
   "metadata": {},
   "source": [
    "# Histogram Equilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfbba53-99f4-44b3-a35e-2aa45a816277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN data\n",
    "for data in dataset_train:\n",
    "    img = cv2.equalizeHist(data[0])\n",
    "    data[0] = img\n",
    "\n",
    "# VAL data\n",
    "for data in dataset_val:\n",
    "    img = cv2.equalizeHist(data[0])\n",
    "    data[0] = img\n",
    "\n",
    "# TEST data\n",
    "for data in dataset_test:\n",
    "    img = cv2.equalizeHist(data[0])\n",
    "    data[0] = img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20e685-aed8-42e6-a1f4-5f7879eacaf6",
   "metadata": {},
   "source": [
    "# Image smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660dd876-a00e-41a4-ac99-177f7388662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bilateral filtering\n",
    "diameter = 3\n",
    "sigma_color = 25\n",
    "sigma_space = 25\n",
    "\n",
    "# TRAIN data\n",
    "for data in dataset_train:\n",
    "    img = cv2.bilateralFilter(data[0], diameter, sigma_color, sigma_space)\n",
    "    data[0] = img\n",
    "\n",
    "#plt.imshow(dataset_train[0][0])\n",
    "#plt.show()\n",
    "\n",
    "# VAL data\n",
    "for data in dataset_val:\n",
    "    img = cv2.bilateralFilter(data[0], diameter, sigma_color, sigma_space)\n",
    "    data[0] = img\n",
    "\n",
    "#plt.imshow(dataset_val[0][0])\n",
    "#plt.show()\n",
    "\n",
    "# TEST data\n",
    "for data in dataset_test:\n",
    "    img = cv2.bilateralFilter(data[0], diameter, sigma_color, sigma_space)\n",
    "    data[0] = img\n",
    "\n",
    "#plt.imshow(dataset_test[0][0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495a95f-82d1-4f3d-b4bb-3a3050629147",
   "metadata": {},
   "source": [
    "# Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae1b0d0-8818-4ce8-b3c0-e2ed7cab97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using min-max scaling\n",
    "\n",
    "# TRAIN data\n",
    "for data in dataset_train:\n",
    "    img = data[0]/255.0\n",
    "    data[0] = img\n",
    "\n",
    "#plt.imshow(dataset_train[0][0])\n",
    "#plt.show()\n",
    "\n",
    "# VAL data\n",
    "for data in dataset_val:\n",
    "    img = data[0]/255.0\n",
    "    data[0] = img\n",
    "\n",
    "#plt.imshow(dataset_val[0][0])\n",
    "#plt.show()\n",
    "\n",
    "# TEST data\n",
    "for data in dataset_test:\n",
    "    img = data[0]/255.0\n",
    "    data[0] = img\n",
    "\n",
    "#plt.imshow(dataset_test[0][0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c75101-ca28-4942-b6dc-a232475733a8",
   "metadata": {},
   "source": [
    "# Morphological operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab29fc7-d25f-442a-92ab-657916eec228",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\thresh.cpp:1559: error: (-2:Unspecified error) in function 'double __cdecl cv::threshold(const class cv::_InputArray &,const class cv::_OutputArray &,double,double,int)'\n> THRESH_OTSU mode:\n>     'src_type == CV_8UC1 || src_type == CV_16UC1'\n> where\n>     'src_type' is 6 (CV_64FC1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m kernel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset_train:\n\u001b[1;32m----> 5\u001b[0m     binr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(data[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY\u001b[38;5;241m+\u001b[39mcv2\u001b[38;5;241m.\u001b[39mTHRESH_OTSU)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m     invert \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbitwise_not(binr)\n\u001b[0;32m      7\u001b[0m     cleaned \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmorphologyEx(invert, cv2\u001b[38;5;241m.\u001b[39mMORPH_OPEN, kernel) \u001b[38;5;66;03m# remove noise(erotion->dilation)\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\thresh.cpp:1559: error: (-2:Unspecified error) in function 'double __cdecl cv::threshold(const class cv::_InputArray &,const class cv::_OutputArray &,double,double,int)'\n> THRESH_OTSU mode:\n>     'src_type == CV_8UC1 || src_type == CV_16UC1'\n> where\n>     'src_type' is 6 (CV_64FC1)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN data\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "for data in dataset_train:\n",
    "    binr = cv2.threshold(data[0], 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "    invert = cv2.bitwise_not(binr)\n",
    "    cleaned = cv2.morphologyEx(invert, cv2.MORPH_OPEN, kernel) # remove noise(erotion->dilation)\n",
    "    filled = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel) # fill small holes\n",
    "    edges = cv2.morphologyEx(filled, cv2.MORPH_GRADIENT, kernel) # get edges\n",
    "    data[0] = edges\n",
    "\n",
    "# VAL data\n",
    "for data in dataset_val:\n",
    "    binr = cv2.threshold(data[0], 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "    invert = cv2.bitwise_not(binr)\n",
    "    cleaned = cv2.morphologyEx(invert, cv2.MORPH_OPEN, kernel)  # Remove noise\n",
    "    filled = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)  # Fill small holes\n",
    "    edges = cv2.morphologyEx(filled, cv2.MORPH_GRADIENT, kernel)  # Get edges\n",
    "    data[0] = edges\n",
    "\n",
    "# TEST data\n",
    "for data in dataset_test:\n",
    "    binr = cv2.threshold(data[0], 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "    invert = cv2.bitwise_not(binr)\n",
    "    erosion = cv2.erode(invert, cv2.MORPH_OPEN, kernel)\n",
    "    cleaned = cv2.morphologyEx(invert, cv2.MORPH_OPEN, kernel)  # Remove noise\n",
    "    filled = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)  # Fill small holes\n",
    "    edges = cv2.morphologyEx(filled, cv2.MORPH_GRADIENT, kernel)  # Get edges\n",
    "    data[0] = edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a468f39-abb2-4f02-9f5b-cbec26b64326",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a27a17-6185-4862-8a49-096774a87433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = adenocarcinoma\n",
      "1 = large_cell_carcinoma\n",
      "2 = normal\n",
      "3 = squamous_cell_carcinoma\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = [], []\n",
    "x_val, y_val = [], []\n",
    "x_test, y_test = [], []\n",
    "\n",
    "for image, category in dataset_train:\n",
    "    x_train.append(image)\n",
    "    y_train.append(category)\n",
    "\n",
    "for image, category in dataset_val:\n",
    "    x_val.append(image)\n",
    "    y_val.append(category)\n",
    "\n",
    "for image, category in dataset_test:\n",
    "    x_test.append(image)\n",
    "    y_test.append(category)\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# apply one-hot encoding\n",
    "\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "for i in range(0, len(label_encoder.classes_)):\n",
    "    print(i, end=\"\")\n",
    "    print(\" = \", end=\"\")\n",
    "    print(label_encoder.classes_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e2b1f-d3ba-4e57-abd7-ead56d38ca14",
   "metadata": {},
   "source": [
    "# Handling data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d96dd02e-2595-481c-b609-ad5e9e92ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613, 256, 256)\n",
      "before:  Counter({0: 195, 3: 155, 2: 148, 1: 115})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivans\\anaconda3\\Lib\\site-packages\\albumentations\\core\\validation.py:45: UserWarning: TimeReverse is an alias for HorizontalFlip transform. Consider using HorizontalFlip directly from albumentations.HorizontalFlip. \n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80347136,)\n",
      "after:  Counter({0: 390, 3: 310, 2: 296, 1: 230})\n"
     ]
    }
   ],
   "source": [
    "# use data augmentation (albumentation library)\n",
    "print(x_train.shape)\n",
    "print(\"before: \", Counter(y_train))\n",
    "\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.8),\n",
    "    A.VerticalFlip(p=0.8),\n",
    "    A.SafeRotate(p=0.8),\n",
    "    A.TimeReverse(p=0.8),\n",
    "    A.GridDistortion(p=0.8),\n",
    "    A.ElasticTransform(p=0.8)\n",
    "])\n",
    "\n",
    "augmented_x_train = []\n",
    "augmented_y_train = []\n",
    "\n",
    "for train_img, train_label in zip(x_train, y_train):\n",
    "    augmented_image = transform(image=train_img)[\"image\"]\n",
    "    augmented_x_train.append(augmented_image)\n",
    "    augmented_y_train.append(train_label)\n",
    "\n",
    "x_train = np.append(x_train, augmented_x_train)\n",
    "y_train = np.append(y_train, augmented_y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(\"after: \", Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7fe5a1-58e3-4d5d-9d72-43f3de60a42d",
   "metadata": {},
   "source": [
    "# Data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5b2a3-85cb-42a7-b633-b05949bd3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_evaluation(y_test_labels, y_pred_labels, model, epoch_num, batch, is_tuning=False):\n",
    "    output_path = 'output2\\\\' + model + '_epoch' + str(epoch_num) + '_batch' + str(batch)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    # Accuracy, precision, recall, f1 score\n",
    "    print(metrics.classification_report(y_test_labels, y_pred_labels, digits=4))\n",
    "    report = metrics.classification_report(y_test_labels, y_pred_labels, digits=4)\n",
    "    cm = metrics.confusion_matrix(y_test_labels, y_pred_labels)\n",
    "    \n",
    "    print(f\"The Cohen's Kappa Score is: {metrics.cohen_kappa_score(y_test_labels, y_pred_labels)}\")\n",
    "    cohen = metrics.cohen_kappa_score(y_test_labels, y_pred_labels)\n",
    "    with open(output_path + '\\\\evaluation_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "        f.write('\\n')\n",
    "        f.write('The Cohen\\'s Kappa Score is: ' + str(cohen))\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Adenocarcinoma', 'Large_cell_carcinoma', 'normal', 'squamous_cell_carcinoma'],\n",
    "                yticklabels=['Adenocarcinoma', 'Large_cell_carcinoma', 'normal', 'squamous_cell_carcinoma'])\n",
    "    plt.xlabel(\"Predicted Labels\", fontsize=14)\n",
    "    plt.ylabel(\"True Labels\", fontsize=14)\n",
    "    plt.savefig(output_path + '\\\\confusion_matrix.jpg', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5092d-cb37-4bb0-8790-92097d5606a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
